To find duplicates in a list in Python, there are several ways to do so, depending on the requirements (e.g., whether you need to count duplicates, just identify them, or remove them). Below are various optimized and alternative methods to find duplicates in a list.

### 1. **Using a Set to Track Seen Elements (Efficient)**

The most efficient way to find duplicates in a list is to use a set. As you iterate over the list, you can check if an element has already been seen (i.e., if itâ€™s in the set). This method runs in **O(n)** time complexity, where `n` is the length of the list.

```python
def find_duplicates(lst: list) -> list:
    seen = set()
    duplicates = []
    for item in lst:
        if item in seen:
            duplicates.append(item)
        else:
            seen.add(item)
    return duplicates

# Example usage
numbers = [1, 2, 3, 4, 5, 1, 3, 6]
print(find_duplicates(numbers))  # Output: [1, 3]
```

**Time Complexity**: O(n)  
**Space Complexity**: O(n)

This method efficiently finds all duplicates in a list.

---

### 2. **Using a Dictionary to Count Occurrences**

You can use a dictionary (or `collections.Counter`) to count occurrences of each element and then return the elements that appear more than once. This is useful if you also want to know how many times each element appears.

```python
from collections import Counter

def find_duplicates(lst: list) -> list:
    counts = Counter(lst)
    return [item for item, count in counts.items() if count > 1]

# Example usage
numbers = [1, 2, 3, 4, 5, 1, 3, 6]
print(find_duplicates(numbers))  # Output: [1, 3]
```

**Time Complexity**: O(n)  
**Space Complexity**: O(n)

This approach provides both the list of duplicates and the count of each element if needed.

---

### 3. **Using a Set to Identify Duplicates (Without Preserving Order)**

If you want to find duplicates without preserving their order, you can modify the earlier approach with a set. This method is also **O(n)** in time complexity.

```python
def find_duplicates(lst: list) -> set:
    seen = set()
    duplicates = set()
    for item in lst:
        if item in seen:
            duplicates.add(item)
        else:
            seen.add(item)
    return duplicates

# Example usage
numbers = [1, 2, 3, 4, 5, 1, 3, 6]
print(find_duplicates(numbers))  # Output: {1, 3}
```

**Time Complexity**: O(n)  
**Space Complexity**: O(n)

This approach is similar to the previous one but returns a set instead of a list and does not preserve the order of duplicates.

---

### 4. **Using List Comprehension (Compact)**

If you prefer a compact solution using list comprehension, you can check for duplicates while iterating through the list and append them only if they are already seen.

```python
def find_duplicates(lst: list) -> list:
    seen = set()
    return [item for i, item in enumerate(lst) if item in seen or seen.add(item)]

# Example usage
numbers = [1, 2, 3, 4, 5, 1, 3, 6]
print(find_duplicates(numbers))  # Output: [1, 3]
```

**Time Complexity**: O(n)  
**Space Complexity**: O(n)

This is a Pythonic one-liner that combines both the tracking of seen elements and finding duplicates in one go.

---

### 5. **Using `filter()` Function**

You can also use the `filter()` function along with a lambda function to identify duplicates in the list. This approach is less common but still functional.

```python
def find_duplicates(lst: list) -> list:
    seen = set()
    return list(filter(lambda x: x in seen or seen.add(x), lst))

# Example usage
numbers = [1, 2, 3, 4, 5, 1, 3, 6]
print(find_duplicates(numbers))  # Output: [1, 3]
```

**Time Complexity**: O(n)  
**Space Complexity**: O(n)

This is a functional programming approach and is similar to the list comprehension method, but it uses `filter()` instead.

---

### 6. **Using `numpy` (For Numeric Lists)**

If you're working with large numerical datasets, you might want to use `numpy` to identify duplicates efficiently. This method is especially useful for numerical data, and `numpy` performs well with large datasets.

```python
import numpy as np

def find_duplicates(lst: list) -> list:
    arr = np.array(lst)
    unique, counts = np.unique(arr, return_counts=True)
    return unique[counts > 1]

# Example usage
numbers = [1, 2, 3, 4, 5, 1, 3, 6]
print(find_duplicates(numbers))  # Output: [1 3]
```

**Time Complexity**: O(n log n) (due to sorting)  
**Space Complexity**: O(n)

`numpy`'s `unique()` function is fast for numerical lists but involves sorting, which results in O(n log n) time complexity. It's optimal for large datasets with numerical values.

---

### Summary of Methods

1. **Using Set**: Fastest and most space-efficient (O(n) time and space complexity). Finds duplicates in linear time.
2. **Using `Counter` (Dictionary)**: Useful if you want to count the occurrences of each element along with finding duplicates.
3. **Using Set for Unordered Duplicates**: A simple and efficient way to find duplicates without preserving their order.
4. **List Comprehension**: Compact and Pythonic approach but may be less readable than using a set.
5. **Using `filter()`**: Another functional programming approach that is concise.
6. **Using `numpy`**: Best suited for numerical lists and large datasets but involves sorting (O(n log n)).

### Best Option

For most general cases, using a **set** (either with a manual loop or in combination with list comprehension) is the **most efficient approach** with O(n) time and space complexity. If you need the count of duplicates, then **`collections.Counter`** is a great choice.
